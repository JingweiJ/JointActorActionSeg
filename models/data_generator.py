import sys
sys.path.append('..')
import utils.matterport_utils as matterport_utils
from models.model_misc import *
import numpy as np
import random
import os
import os.path as osp
import pickle


def load_image_gt(dataset, config, image_id, augment=False,
                  use_mini_mask=False):
    """Load and return ground truth data for an image (image, mask, bounding boxes).

    augment: If true, apply random image augmentation. Currently, only
        horizontal flipping is offered.
    use_mini_mask: If False, returns full-size masks that are the same height
        and width as the original image. These can be big, for example
        1024x1024x100 (for 100 instances). Mini masks are smaller, typically,
        224x224 and are generated by extracting the bounding box of the
        object and resizing it to MINI_MASK_SHAPE.

    Returns:
    image: [height, width, 3]
    image_meta: meta info of image
    rs_rgb_clip: [TIMESTEPS, height, width, 3], resized clip of rgb image
    rs_flow_clip: [TIMESTEPS, height, width, 2], resized clip of flow
    labeled_frame_id: [TIMESTEPS], one-hot vector indicating the location of this labeled frame in clip.
    bbox: [instance_count, (y1, x1, y2, x2, actor_class_id, action_class_id)]
    mask: [height, width, instance_count]. The height and width are those
        of the image unless use_mini_mask is True, in which case they are
        defined in MINI_MASK_SHAPE.
    """
    # Load image and mask
    image = dataset.load_image(image_id)
    mask, actor_class_ids, action_class_ids = dataset.load_mask(image_id)
    rgb_clip, flow_clip, labeled_frame_id = dataset.load_clip(image_id, config.TIMESTEPS) # TODO: implement load_clip
    one_hot = np.zeros((config.TIMESTEPS,))
    one_hot[labeled_frame_id] = 1
    labeled_frame_id = one_hot
    # rgb_clip: (TIMESTEPS, height, width, 3), flow_clip: (TIMESTEPS, height, width, 2), labeled_frame_id: integer
    shape = image.shape
    image, window, scale, padding = matterport_utils.resize_image(
        image, 
        min_dim=config.IMAGE_MIN_DIM, 
        max_dim=config.IMAGE_MAX_DIM,
        padding=config.IMAGE_PADDING)
    mask = matterport_utils.resize_mask(mask, scale, padding)

    rs_rgb_clip, rs_flow_clip = [], []
    for i in range(config.TIMESTEPS):
        rs_rgb_clip.append(matterport_utils.resize_image(
            rgb_clip[i,...],
            min_dim=config.IMAGE_MIN_DIM,
            max_dim=config.IMAGE_MAX_DIM,
            padding=config.IMAGE_PADDING)[0])
        rs_flow_clip.append(matterport_utils.resize_image( # TODO: test
            flow_clip[i,...],
            min_dim=config.IMAGE_MIN_DIM,
            max_dim=config.IMAGE_MAX_DIM,
            padding=config.IMAGE_PADDING,
            mode='flow')[0])
    rs_rgb_clip = np.array(rs_rgb_clip)
    rs_flow_clip = np.array(rs_flow_clip)


    # Random horizontal flips.
    if augment:
        if random.randint(0, 1):
            image = np.fliplr(image)
            mask = np.fliplr(mask)
            rs_rgb_clip = np.transpose(np.fliplr(np.transpose(rs_rgb_clip, (1,2,3,0))), (3,0,1,2))
            rs_flow_clip = np.transpose(np.fliplr(np.transpose(rs_flow_clip, (1,2,3,0))), (3,0,1,2))

    # Bounding boxes. Note that some boxes might be all zeros
    # if the corresponding mask got cropped out.
    # bbox: [num_instances, (y1, x1, y2, x2)]
    bbox = matterport_utils.extract_bboxes(mask)

    # Add class_id as the last value in bbox
    bbox = np.hstack([bbox, actor_class_ids[:, np.newaxis], action_class_ids[:, np.newaxis]])

    # Active classes
    # Different datasets have different classes, so track the
    # classes supported in the dataset of this image.
    active_actor_class_ids = np.zeros([dataset.num_actor_classes], dtype=np.int32)
    class_ids = dataset.actor_source_class_ids[dataset.image_info[image_id]["source"]]
    active_actor_class_ids[class_ids] = 1

    active_action_class_ids = np.zeros([dataset.num_action_classes], dtype=np.int32)
    class_ids = dataset.action_source_class_ids[dataset.image_info[image_id]["source"]]
    active_action_class_ids[class_ids] = 1

    # Resize masks to smaller size to reduce memory usage
    if use_mini_mask:
        mask = matterport_utils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)

    # Image meta data
    image_meta = compose_image_meta(image_id, shape, window, active_actor_class_ids, active_action_class_ids)

    return image, image_meta, rs_rgb_clip, rs_flow_clip, labeled_frame_id, bbox, mask


def load_image_gt_preprocessed(dataset, config, image_id, augment=False):
    # Load image, flow, image_meta, image_path
    video_id = dataset.image_info[image_id]['path'].split('/')[-2]
    frame_id = int(dataset.image_info[image_id]['path'].split('/')[-1].rstrip('.png'))
    prep_path = os.path.join(dataset.dataset_dir, 'Preprocessed', video_id, '%05d.pkl' % frame_id)
    with open(prep_path, 'rb') as f:
        prep = pickle.load(f)
    image, flow, image_meta, bbox, mask = prep['image'], prep['flow'], prep['image_meta'], prep['bbox'], prep['mask']
    image_meta[0] = image_id

    rgb_clip, flow_clip, labeled_frame_id = dataset.load_clip(image_id, config.TIMESTEPS, preprocessed=True)
    one_hot = np.zeros((config.TIMESTEPS,))
    one_hot[labeled_frame_id] = 1
    labeled_frame_id = one_hot

    if augment:
        if random.randint(0, 1):
            image = np.fliplr(image)
            mask = np.fliplr(mask)
            rgb_clip = np.transpose(np.fliplr(np.transpose(rgb_clip, (1,2,3,0))), (3,0,1,2))
            flow_clip = np.transpose(np.fliplr(np.transpose(flow_clip, (1,2,3,0))), (3,0,1,2))

    return  image, image_meta, rgb_clip, flow_clip, labeled_frame_id, bbox, mask


def build_detection_targets(rpn_rois, gt_boxes, gt_masks, config):
    """Generate targets for training Stage 2 classifier and mask heads.

    Inputs:
    rpn_rois: [N, (y1, x1, y2, x2)] proposal boxes.
    gt_boxes: [instance count, (y1, x1, y2, x2, class_id)]
    gt_masks: [height, width, instance count] Grund truth masks. Can be full
              size or mini-masks.

    Returns:
    rois: [TRAIN_ROIS_PER_IMAGE, (y1, x1, y2, x2)]
    class_ids: [TRAIN_ROIS_PER_IMAGE]. Int class IDs.
    bboxes: [TRAIN_ROIS_PER_IMAGE, NUM_CLASSES, 5]. Rows are class-specific 
            bbox refinments [y, x, log(h), log(w), weight].
    masks: [TRAIN_ROIS_PER_IMAGE, height, width, NUM_CLASSES). Class specific masks cropped
           to bbox boundaries and resized to neural network output size.
    """
    assert rpn_rois.shape[0] > 0
    assert gt_boxes.dtype == np.int32, "Expected int but got {}".format(gt_boxes.dtype)
    assert gt_masks.dtype == np.bool_, "Expected bool but got {}".format(gt_masks.dtype)

    # It's common to add GT Boxes to ROIs but we don't do that here because
    # according to XinLei Chen's paper, it doesn't help.

    # Trim empty padding in gt_boxes and gt_masks parts
    instance_ids = np.where(gt_boxes[:, 4] > 0)[0]
    assert instance_ids.shape[0] > 0, "Image must contain instances."
    gt_boxes = gt_boxes[instance_ids]
    gt_masks = gt_masks[:, :, instance_ids]

    # Compute areas of ROIs and ground truth boxes.
    rpn_roi_area = (rpn_rois[:, 2] - rpn_rois[:, 0]) * (rpn_rois[:, 3] - rpn_rois[:, 1])
    gt_box_area = (gt_boxes[:, 2] - gt_boxes[:, 0]) * (gt_boxes[:, 3] - gt_boxes[:, 1])

    # Compute overlaps [rpn_rois, gt_boxes]
    overlaps = np.zeros((rpn_rois.shape[0], gt_boxes.shape[0]))
    for i in range(overlaps.shape[1]):
        gt = gt_boxes[i][:4]
        overlaps[:,i] = matterport_utils.compute_iou(gt, rpn_rois, gt_box_area[i], rpn_roi_area)

    # Assign ROIs to GT boxes
    rpn_roi_iou_argmax = np.argmax(overlaps, axis=1)
    rpn_roi_iou_max = overlaps[np.arange(overlaps.shape[0]), rpn_roi_iou_argmax]
    rpn_roi_gt_boxes = gt_boxes[rpn_roi_iou_argmax]  # GT box assigned to each ROI

    # Positive ROIs are those with >= 0.5 IoU with a GT box. 
    fg_ids = np.where(rpn_roi_iou_max > 0.5)[0]

    # Negative ROIs are those with max IoU 0.1-0.5 (hard example mining)
    # TODO: To hard example mine or not to hard example mine, that's the question
#     bg_ids = np.where((rpn_roi_iou_max >= 0.1) & (rpn_roi_iou_max < 0.5))[0]
    bg_ids = np.where(rpn_roi_iou_max < 0.5)[0]

    # Subsample ROIs. Aim for 33% foreground.
    # FG
    fg_roi_count = int(config.TRAIN_ROIS_PER_IMAGE * config.ROI_POSITIVE_RATIO)
    if fg_ids.shape[0] > fg_roi_count:
        keep_fg_ids = np.random.choice(fg_ids, fg_roi_count, replace=False)
    else:
        keep_fg_ids = fg_ids
    # BG
    remaining = config.TRAIN_ROIS_PER_IMAGE - keep_fg_ids.shape[0]
    if bg_ids.shape[0] > remaining:
        keep_bg_ids = np.random.choice(bg_ids, remaining, replace=False)
    else:
        keep_bg_ids = bg_ids
    # Combine indicies of ROIs to keep
    keep = np.concatenate([keep_fg_ids, keep_bg_ids])
    # Need more?
    remaining = config.TRAIN_ROIS_PER_IMAGE - keep.shape[0]
    if remaining > 0:
        # Looks like we don't have enough samples to maintain the desired
        # balance. Reduce requirements and fill in the rest. This is
        # likely different from the Mask RCNN paper.

        # There is a small chance we have neither fg nor bg samples.
        if keep.shape[0] == 0:
            # Pick bg regions with easier IoU threshold
            bg_ids = np.where(rpn_roi_iou_max < 0.5)[0]
            assert bg_ids.shape[0] >= remaining
            keep_bg_ids = np.random.choice(bg_ids, remaining, replace=False)
            assert keep_bg_ids.shape[0] == remaining
            keep = np.concatenate([keep, keep_bg_ids])
        else:
            # Fill the rest with repeated bg rois.
            keep_extra_ids = np.random.choice(keep_bg_ids, remaining, replace=True)
            keep = np.concatenate([keep, keep_extra_ids])
    assert keep.shape[0] == config.TRAIN_ROIS_PER_IMAGE, \
        "keep doesn't match ROI batch size {}, {}".format(
            keep.shape[0], config.TRAIN_ROIS_PER_IMAGE)

    # Reset the gt boxes assigned to BG ROIs.
    rpn_roi_gt_boxes[keep_bg_ids, :] = 0

    # For each kept ROI, assign a class_id, and for FG ROIs also add bbox refinement.
    rois = rpn_rois[keep, :4]
    roi_gt_boxes = rpn_roi_gt_boxes[keep]
    class_ids = roi_gt_boxes[:,4].astype(np.int32)
    roi_gt_assignment = rpn_roi_iou_argmax[keep]

    # Class-aware bbox shifts. [y, x, log(h), log(w), weight]. Weight is 0 or 1 to
    # determine if a bbox is included in the loss.
    bboxes = np.zeros((config.TRAIN_ROIS_PER_IMAGE, config.NUM_CLASSES, 5), dtype=np.float32)
    pos_ids = np.where(class_ids > 0)[0]
    bboxes[pos_ids, class_ids[pos_ids], :4] = matterport_utils.box_refinement(rois[pos_ids], roi_gt_boxes[pos_ids, :4])
    bboxes[pos_ids, class_ids[pos_ids], 4] = 1  # weight = 1 to influence the loss
    # Normalize bbox refinments
    bboxes[:, :, :4] /= config.BBOX_STD_DEV

    # Generate class-specific target masks.
    masks = np.zeros((config.TRAIN_ROIS_PER_IMAGE, config.MASK_SHAPE[0], config.MASK_SHAPE[1], config.NUM_CLASSES), 
                     dtype=np.float32)
    for i in pos_ids:
        class_id = class_ids[i]
        assert class_id > 0, "class id must be greater than 0"
        gt_id = roi_gt_assignment[i]
        class_mask = gt_masks[:, :, gt_id]
        
        if config.USE_MINI_MASK:
            # Create a mask placeholder, the size of the image
            placeholder = np.zeros(config.IMAGE_SHAPE[:2], dtype=bool)
            # GT box
            gt_y1, gt_x1, gt_y2, gt_x2 = gt_boxes[gt_id][:4]
            gt_w = gt_x2 - gt_x1
            gt_h = gt_y2 - gt_y1
            # Resize mini mask to size of GT box
            placeholder[gt_y1:gt_y2, gt_x1:gt_x2] = \
                np.round(scipy.misc.imresize(class_mask.astype(float), (gt_h, gt_w), 
                                             interp='nearest') / 255.0).astype(bool)
            # Place the mini batch in the placeholder
            class_mask = placeholder
            
        # Pick part of the mask and resize it
        y1, x1, y2, x2 = rois[i][:4].astype(np.int32)
        m = class_mask[y1:y2, x1:x2]
        mask = scipy.misc.imresize(m.astype(float), config.MASK_SHAPE, interp='nearest') / 255.0
        masks[i,:,:,class_id] = mask
        
    return rois, class_ids, bboxes, masks


def build_rpn_targets(image_shape, anchors, gt_boxes, config):
    """Given the anchors and GT boxes, compute overlaps and identify positive
    anchors and deltas to refine them to match their corresponding GT boxes.

    anchors: [num_anchors, (y1, x1, y2, x2)]
    gt_boxes: [num_gt_boxes, (y1, x1, y2, x2, class_id)]

    Returns:
    rpn_match: [N] (int32) matches between anchors and GT boxes.
               1 = positive anchor, -1 = negative anchor, 0 = neutral
    rpn_bbox: [N, (dy, dx, log(dh), log(dw))] Anchor bbox deltas.
    """
    # RPN Match: 1 = positive anchor, -1 = negative anchor, 0 = neutral
    rpn_match = np.zeros([anchors.shape[0]], dtype=np.int32)
    # RPN bounding boxes: [max anchors per image, (dy, dx, log(dh), log(dw))]
    rpn_bbox = np.zeros((config.RPN_TRAIN_ANCHORS_PER_IMAGE, 4))

    # Areas of anchors and GT boxes
    gt_box_area = (gt_boxes[:, 2] - gt_boxes[:, 0]) * (gt_boxes[:, 3] - gt_boxes[:, 1])
    anchor_area = (anchors[:, 2] - anchors[:, 0]) * (anchors[:, 3] - anchors[:, 1])

    # Compute overlaps [num_anchors, num_gt_boxes]
    # Each cell contains the IoU of an anchor and GT box.
    overlaps = np.zeros((anchors.shape[0], gt_boxes.shape[0]))
    for i in range(overlaps.shape[1]):
        gt = gt_boxes[i][:4]
        overlaps[:,i] = matterport_utils.compute_iou(gt, anchors, gt_box_area[i], anchor_area)

    # Match anchors to GT Boxes
    # If an anchor overlaps a GT box with IoU >= 0.7 then it's positive.
    # If an anchor overlaps a GT box with IoU < 0.3 then it's negative.
    # Neutral anchors are those that don't match the conditions above, 
    # and they don't influence the loss function.
    # However, don't keep any GT box unmatched (rare, but happens). Instead,
    # match it to the closest anchor (even if its max IoU is < 0.3).
    #
    # 1. Set negative anchors first. It gets overwritten if a gt box is matched to them.
    anchor_iou_argmax = np.argmax(overlaps, axis=1)
    anchor_iou_max = overlaps[np.arange(overlaps.shape[0]), anchor_iou_argmax]
    rpn_match[anchor_iou_max < 0.3] = -1
    # 2. Set an anchor for each GT box (regardless of IoU value).
    # TODO: If multiple anchors have the same IoU match all of them
    gt_iou_argmax = np.argmax(overlaps, axis=0)
    rpn_match[gt_iou_argmax] = 1
    # 3. Set anchors with high overlap as positive.
    rpn_match[anchor_iou_max >= 0.7] = 1

    # Subsample to balance positive and negative anchors
    # Don't let positives be more than half the anchors
    ids = np.where(rpn_match == 1)[0]
    extra = len(ids) - (config.RPN_TRAIN_ANCHORS_PER_IMAGE // 2)
    if extra > 0:
        # Reset the extra ones to neutral
        ids = np.random.choice(ids, extra, replace=False)
        rpn_match[ids] = 0
    # Same for negative proposals
    ids = np.where(rpn_match == -1)[0]
    extra = len(ids) - (config.RPN_TRAIN_ANCHORS_PER_IMAGE - np.sum(rpn_match == 1))
    if extra > 0:
        # Rest the extra ones to neutral
        ids = np.random.choice(ids, extra, replace=False)
        rpn_match[ids] = 0

    # For positive anchors, compute shift and scale needed to transform them
    # to match the corresponding GT boxes.
    ids = np.where(rpn_match == 1)[0]
    ix = 0  # index into rpn_bbox
    # TODO: use box_refinment() rather that duplicating the code here
    for i, a in zip(ids, anchors[ids]):
        # Closest gt box (it might have IoU < 0.7)
        gt = gt_boxes[anchor_iou_argmax[i], :4]

        # Convert coordinates to center plus width/height.
        # GT Box
        gt_h = gt[2] - gt[0]
        gt_w = gt[3] - gt[1]
        gt_center_y = gt[0] + 0.5 * gt_h
        gt_center_x = gt[1] + 0.5 * gt_w
        # Anchor
        a_h = a[2] - a[0]
        a_w = a[3] - a[1]
        a_center_y = a[0] + 0.5 * a_h
        a_center_x = a[1] + 0.5 * a_w

        # Compute the bbox refinement that the RPN should predict.
        rpn_bbox[ix] = [
            (gt_center_y - a_center_y) / a_h,
            (gt_center_x - a_center_x) / a_w,
            np.log(gt_h / a_h),
            np.log(gt_w / a_w),
        ]
        # Normalize
        rpn_bbox[ix] /= config.RPN_BBOX_STD_DEV
        ix += 1

    return rpn_match, rpn_bbox


def generate_random_rois(image_shape, count, gt_boxes):
    """Generates ROI proposals similar to what a region proposal network
    would generate.

    image_shape: [Height, Width, Depth]
    count: Number of ROIs to generate
    gt_boxes: [N, (y1, x1, y2, x2, class_id)] Ground trugh boxes in pixels.

    Returns: [count, (y1, x1, y2, x2)] ROI boxes in pixels.
    """
    # placeholder
    rois = np.zeros((count, 4), dtype=np.int32)
    
    # Generate random ROIs around GT boxes (90% of count)
    rois_per_box = int(0.9 * count / gt_boxes.shape[0])
    for i in range(gt_boxes.shape[0]):
        gt_y1, gt_x1, gt_y2, gt_x2 = gt_boxes[i,:4]
        h = gt_y2 - gt_y1
        w = gt_x2 - gt_x1
        # random boundaries
        r_y1 = max(gt_y1-h, 0)
        r_y2 = min(gt_y2+h, image_shape[0])
        r_x1 = max(gt_x1-w, 0)
        r_x2 = min(gt_x2+w, image_shape[1])
        
        # To avoid generating boxes with zero area, we generate double what
        # we need and filter out the extra. If we get fewer valid boxes 
        # than we need, we loop and try again.
        while True:
            y1y2 = np.random.randint(r_y1, r_y2, (rois_per_box*2, 2))
            x1x2 = np.random.randint(r_x1, r_x2, (rois_per_box*2, 2))
            # Filter out zero area boxes
            threshold = 1
            y1y2 = y1y2[np.abs(y1y2[:,0] - y1y2[:,1]) >= threshold][:rois_per_box]
            x1x2 = x1x2[np.abs(x1x2[:,0] - x1x2[:,1]) >= threshold][:rois_per_box]
            if y1y2.shape[0] == rois_per_box and x1x2.shape[0] == rois_per_box:
                break
        
        # Sort on axis 1 to ensure x1 <= x2 and y1 <= y2 and then reshape
        # into x1, y1, x2, y2 order
        x1, x2 = np.split(np.sort(x1x2, axis=1), 2, axis=1)
        y1, y2 = np.split(np.sort(y1y2, axis=1), 2, axis=1)
        box_rois = np.hstack([y1, x1, y2, x2])
        rois[rois_per_box*i:rois_per_box*(i+1)] = box_rois
    
    # Generate random ROIs anywhere in the image (10% of count)
    remaining_count = count - (rois_per_box * gt_boxes.shape[0])
    # To avoid generating boxes with zero area, we generate double what
    # we need and filter out the extra. If we get fewer valid boxes 
    # than we need, we loop and try again.
    while True:
        y1y2 = np.random.randint(0, image_shape[0], (remaining_count * 2, 2))
        x1x2 = np.random.randint(0, image_shape[1], (remaining_count * 2, 2))
        # Filter out zero area boxes
        threshold = 1
        y1y2 = y1y2[np.abs(y1y2[:,0] - y1y2[:,1]) >= threshold][:remaining_count]
        x1x2 = x1x2[np.abs(x1x2[:,0] - x1x2[:,1]) >= threshold][:remaining_count]
        if y1y2.shape[0] == remaining_count and x1x2.shape[0] == remaining_count:
            break
    
    # Sort on axis 1 to ensure x1 <= x2 and y1 <= y2 and then reshape
    # into x1, y1, x2, y2 order
    x1, x2 = np.split(np.sort(x1x2, axis=1), 2, axis=1)
    y1, y2 = np.split(np.sort(y1y2, axis=1), 2, axis=1)
    global_rois = np.hstack([y1, x1, y2, x2])
    rois[-remaining_count:] = global_rois
    return rois


def data_generator(dataset, config, shuffle=True, augment=True, random_rois=0,
                   batch_size=1, detection_targets=False):  # TODO: make sure every arg is picklable
    """A generator that returns images and corresponding target class ids, 
    bounding box deltas, and masks.

    dataset: The A2DDataset object to pick data from
    config: The model config object
    shuffle: If True, shuffles the samples before every epoch
    augment: If True, applies image augmentation to images (currently only 
             horizontal flips are supported)
    random_rois: If > 0 then generate proposals to be used to train the
                 network classifier and mask heads. Useful if training
                 the Mask RCNN part without the RPN.
    batch_size: How many images to return in each call
    detection_targets: If True, generate detection targets (class IDs, bbox
        deltas, and masks). Typically for debugging or visualizations because
        in trainig detection targets are generated by DetectionTargetLayer.

    Returns a Python generator. Upon calling next() on it, the 
    generator returns two lists, inputs and outputs. The containtes
    of the lists differs depending on the received arguments:
    inputs list:
    - images: [batch, H, W, C]
    - image_meta: [batch, size of image meta]
    - rpn_match: [batch, N] Integer (1=positive anchor, -1=negative, 0=neutral)
    - rpn_bbox: [batch, N, (dy, dx, log(dh), log(dw))] Anchor bbox deltas.
    - gt_boxes: [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2, class_id)]
    - gt_masks: [batch, height, width, MAX_GT_INSTANCES]. The height and width
                are those of the image unless use_mini_mask is True, in which
                case they are defined in MINI_MASK_SHAPE.

    outputs list: Usually empty in regular training. But if detection_targets
        is True then the outputs list contains target class_ids, bbox deltas,
        and masks.
    """
    b = 0  # batch item index
    image_index = -1
    image_ids = np.copy(dataset.image_ids)
    error_count = 0

    # Anchors
    # [anchor_count, (y1, x1, y2, x2)]
    anchors = matterport_utils.generate_pyramid_anchors(config.RPN_ANCHOR_SCALES, 
                                              config.RPN_ANCHOR_RATIOS,
                                              config.BACKBONE_SHAPES,
                                              config.BACKBONE_STRIDES, 
                                              config.RPN_ANCHOR_STRIDE)

    # Keras requires a generator to run indefinately.
    while True:
    #try:
        # Increment index to pick next image. Shuffle if at the start of an epoch.
        image_index = (image_index + 1) % len(image_ids)
        if shuffle and image_index == 0:
            np.random.shuffle(image_ids)

        # Get GT bounding boxes and masks for image.
        image_id = image_ids[image_index]

        if config.USE_PREPROCESSED_DATA:
            image, image_meta, rgb_clip, flow_clip, labeled_frame_id, gt_boxes, gt_masks = \
                load_image_gt_preprocessed(dataset, config, image_id, augment=augment)
        else:
            image, image_meta, rgb_clip, flow_clip, labeled_frame_id, gt_boxes, gt_masks = \
                load_image_gt(dataset, config, image_id, augment=augment, use_mini_mask=config.USE_MINI_MASK)

        # Skip images that have no instances. This can happen in cases 
        # where we train on a subset of classes and the image doesn't
        # have any of the classes we care about.
        if np.sum(gt_boxes) <= 0:
            continue

        # RPN Targets
        rpn_match, rpn_bbox = build_rpn_targets(image.shape, anchors, gt_boxes, config)

        # Mask R-CNN Targets
        if random_rois: # TODO: modify for padnet
            rpn_rois = generate_random_rois(image.shape, random_rois, gt_boxes)
            if detection_targets:
                # Append two columns of zeros. TODO: needed?
                rpn_rois = np.hstack([rpn_rois, np.zeros([rpn_rois.shape[0], 2], dtype=np.int32)])
                rois, mrcnn_class_ids, mrcnn_bbox, mrcnn_mask =\
                    build_detection_targets(rpn_rois, gt_boxes, gt_masks, config)

        # Init batch arrays
        if b == 0:
            batch_image_meta = np.zeros((batch_size,)+image_meta.shape, dtype=image_meta.dtype)
            batch_rgb_clip = np.zeros((batch_size,)+rgb_clip.shape, dtype=np.float32)
            batch_flow_clip = np.zeros((batch_size,)+flow_clip.shape, dtype=np.float32)
            batch_labeled_frame_id = np.zeros((batch_size,)+labeled_frame_id.shape, dtype=np.int32)
            batch_rpn_match = np.zeros([batch_size, anchors.shape[0], 1], dtype=rpn_match.dtype)
            batch_rpn_bbox = np.zeros([batch_size, config.RPN_TRAIN_ANCHORS_PER_IMAGE, 4], dtype=rpn_bbox.dtype)
            batch_images = np.zeros((batch_size,)+image.shape, dtype=np.float32)
            batch_gt_boxes = np.zeros((batch_size, config.MAX_GT_INSTANCES, 6), dtype=np.int32)
            if config.USE_MINI_MASK:
                batch_gt_masks = np.zeros((batch_size, config.MINI_MASK_SHAPE[0], config.MINI_MASK_SHAPE[1], 
                                           config.MAX_GT_INSTANCES))
            else:
                batch_gt_masks = np.zeros((batch_size, image.shape[0], image.shape[1], config.MAX_GT_INSTANCES))
            if random_rois: # TODO: not modified for padnet yet
                batch_rpn_rois = np.zeros((batch_size,rpn_rois.shape[0], 4), dtype=rpn_rois.dtype)
                if detection_targets:
                    batch_rois = np.zeros((batch_size,)+rois.shape, dtype=rois.dtype)
                    batch_mrcnn_class_ids = np.zeros((batch_size,)+mrcnn_class_ids.shape, dtype=mrcnn_class_ids.dtype)
                    batch_mrcnn_bbox = np.zeros((batch_size,)+mrcnn_bbox.shape, dtype=mrcnn_bbox.dtype)
                    batch_mrcnn_mask = np.zeros((batch_size,)+mrcnn_mask.shape, dtype=mrcnn_mask.dtype)

        # If more instances than fits in the array, sub-sample from them.
        if gt_boxes.shape[0] > config.MAX_GT_INSTANCES:
            ids = np.random.choice(np.arange(gt_boxes.shape[0]), config.MAX_GT_INSTANCES, replace=False)
            gt_boxes = gt_boxes[ids]
            gt_masks = gt_masks[:,:,ids]

        # Add to batch
        batch_image_meta[b] = image_meta
        batch_rgb_clip[b] = mold_rgb_clip(rgb_clip, config)
        batch_flow_clip[b] = flow_clip
        batch_labeled_frame_id[b] = labeled_frame_id
        batch_rpn_match[b] = rpn_match[:, np.newaxis]
        batch_rpn_bbox[b] = rpn_bbox
        batch_images[b] = mold_image(image.astype(np.float32), config)
        batch_gt_boxes[b,:gt_boxes.shape[0]] = gt_boxes
        batch_gt_masks[b,:,:,:gt_masks.shape[-1]] = gt_masks
        if random_rois: # TODO: not modified for padnet yet
            batch_rpn_rois[b] = rpn_rois[:,:4]
            if detection_targets:
                batch_rois[b] = rois
                batch_mrcnn_class_ids[b] = mrcnn_class_ids
                batch_mrcnn_bbox[b] = mrcnn_bbox
                batch_mrcnn_mask[b] = mrcnn_mask
        b += 1

        # Batch full?
        if b >= batch_size:
            inputs = [batch_rgb_clip, batch_flow_clip, batch_image_meta, batch_labeled_frame_id,
                      batch_rpn_match, batch_rpn_bbox, batch_gt_boxes, batch_gt_masks]
            outputs = []

            if random_rois: # TODO: not modified for padnet yet
                inputs.extend([batch_rpn_rois])
                if detection_targets:
                    inputs.extend([batch_rois])
                    # Keras requires that output and targets have the same number of dimensions
                    batch_mrcnn_class_ids = np.expand_dims(batch_mrcnn_class_ids, -1)
                    outputs.extend([batch_mrcnn_class_ids, batch_mrcnn_bbox, batch_mrcnn_mask])

            yield inputs, outputs

            # start a new batch
            b = 0
        #except (GeneratorExit, KeyboardInterrupt):
        #    raise
        #except:
        #    # Log it and skip the image
        #    logging.exception("Error processing image {}".format(dataset.image_info[image_id]))
        #    error_count += 1
        #    if error_count > 5:
        #        raise

